{
  "best_global_step": 1406,
  "best_metric": 0.7155680656433105,
  "best_model_checkpoint": "./results/checkpoint-1406",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2109,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01422475106685633,
      "grad_norm": 102.72285461425781,
      "learning_rate": 9e-07,
      "loss": 2.192,
      "step": 10
    },
    {
      "epoch": 0.02844950213371266,
      "grad_norm": 111.30522155761719,
      "learning_rate": 1.9e-06,
      "loss": 2.1178,
      "step": 20
    },
    {
      "epoch": 0.04267425320056899,
      "grad_norm": 137.437744140625,
      "learning_rate": 2.9e-06,
      "loss": 2.3881,
      "step": 30
    },
    {
      "epoch": 0.05689900426742532,
      "grad_norm": 54.50032424926758,
      "learning_rate": 3.9e-06,
      "loss": 1.9923,
      "step": 40
    },
    {
      "epoch": 0.07112375533428165,
      "grad_norm": 109.57581329345703,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 1.8383,
      "step": 50
    },
    {
      "epoch": 0.08534850640113797,
      "grad_norm": 76.68930053710938,
      "learning_rate": 5.9e-06,
      "loss": 2.3772,
      "step": 60
    },
    {
      "epoch": 0.09957325746799431,
      "grad_norm": 153.5294647216797,
      "learning_rate": 6.900000000000001e-06,
      "loss": 2.0692,
      "step": 70
    },
    {
      "epoch": 0.11379800853485064,
      "grad_norm": 112.39378356933594,
      "learning_rate": 7.9e-06,
      "loss": 1.9094,
      "step": 80
    },
    {
      "epoch": 0.12802275960170698,
      "grad_norm": 62.70467758178711,
      "learning_rate": 8.9e-06,
      "loss": 1.7679,
      "step": 90
    },
    {
      "epoch": 0.1422475106685633,
      "grad_norm": 74.68572235107422,
      "learning_rate": 9.900000000000002e-06,
      "loss": 1.6312,
      "step": 100
    },
    {
      "epoch": 0.15647226173541964,
      "grad_norm": 69.95448303222656,
      "learning_rate": 1.09e-05,
      "loss": 1.3155,
      "step": 110
    },
    {
      "epoch": 0.17069701280227595,
      "grad_norm": 28.686546325683594,
      "learning_rate": 1.19e-05,
      "loss": 1.1616,
      "step": 120
    },
    {
      "epoch": 0.18492176386913228,
      "grad_norm": 32.283443450927734,
      "learning_rate": 1.29e-05,
      "loss": 1.0691,
      "step": 130
    },
    {
      "epoch": 0.19914651493598862,
      "grad_norm": 18.37740707397461,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.8809,
      "step": 140
    },
    {
      "epoch": 0.21337126600284495,
      "grad_norm": 25.764705657958984,
      "learning_rate": 1.49e-05,
      "loss": 0.8297,
      "step": 150
    },
    {
      "epoch": 0.22759601706970128,
      "grad_norm": 19.178476333618164,
      "learning_rate": 1.59e-05,
      "loss": 0.8296,
      "step": 160
    },
    {
      "epoch": 0.24182076813655762,
      "grad_norm": 20.339519500732422,
      "learning_rate": 1.69e-05,
      "loss": 0.9875,
      "step": 170
    },
    {
      "epoch": 0.25604551920341395,
      "grad_norm": 12.687300682067871,
      "learning_rate": 1.79e-05,
      "loss": 0.9249,
      "step": 180
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 17.799774169921875,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.9205,
      "step": 190
    },
    {
      "epoch": 0.2844950213371266,
      "grad_norm": 13.774762153625488,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.8126,
      "step": 200
    },
    {
      "epoch": 0.29871977240398295,
      "grad_norm": 20.844934463500977,
      "learning_rate": 2.09e-05,
      "loss": 0.9074,
      "step": 210
    },
    {
      "epoch": 0.3129445234708393,
      "grad_norm": 17.772987365722656,
      "learning_rate": 2.19e-05,
      "loss": 0.9204,
      "step": 220
    },
    {
      "epoch": 0.32716927453769556,
      "grad_norm": 8.631269454956055,
      "learning_rate": 2.29e-05,
      "loss": 0.8262,
      "step": 230
    },
    {
      "epoch": 0.3413940256045519,
      "grad_norm": 12.559026718139648,
      "learning_rate": 2.39e-05,
      "loss": 0.9056,
      "step": 240
    },
    {
      "epoch": 0.35561877667140823,
      "grad_norm": 14.148594856262207,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.7703,
      "step": 250
    },
    {
      "epoch": 0.36984352773826457,
      "grad_norm": 15.291461944580078,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.7998,
      "step": 260
    },
    {
      "epoch": 0.3840682788051209,
      "grad_norm": 19.07378387451172,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.7921,
      "step": 270
    },
    {
      "epoch": 0.39829302987197723,
      "grad_norm": 11.652397155761719,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.8548,
      "step": 280
    },
    {
      "epoch": 0.41251778093883357,
      "grad_norm": 16.982351303100586,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.8884,
      "step": 290
    },
    {
      "epoch": 0.4267425320056899,
      "grad_norm": 11.49599838256836,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.7785,
      "step": 300
    },
    {
      "epoch": 0.44096728307254623,
      "grad_norm": 17.605920791625977,
      "learning_rate": 3.09e-05,
      "loss": 0.8013,
      "step": 310
    },
    {
      "epoch": 0.45519203413940257,
      "grad_norm": 15.641258239746094,
      "learning_rate": 3.19e-05,
      "loss": 0.8012,
      "step": 320
    },
    {
      "epoch": 0.4694167852062589,
      "grad_norm": 15.565929412841797,
      "learning_rate": 3.29e-05,
      "loss": 0.7945,
      "step": 330
    },
    {
      "epoch": 0.48364153627311524,
      "grad_norm": 11.48807430267334,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.8437,
      "step": 340
    },
    {
      "epoch": 0.49786628733997157,
      "grad_norm": 11.780911445617676,
      "learning_rate": 3.49e-05,
      "loss": 0.7726,
      "step": 350
    },
    {
      "epoch": 0.5120910384068279,
      "grad_norm": 14.813543319702148,
      "learning_rate": 3.59e-05,
      "loss": 0.8459,
      "step": 360
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 11.437114715576172,
      "learning_rate": 3.69e-05,
      "loss": 0.8374,
      "step": 370
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 9.788081169128418,
      "learning_rate": 3.79e-05,
      "loss": 0.8724,
      "step": 380
    },
    {
      "epoch": 0.5547652916073968,
      "grad_norm": 12.076562881469727,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.7948,
      "step": 390
    },
    {
      "epoch": 0.5689900426742532,
      "grad_norm": 11.19809627532959,
      "learning_rate": 3.99e-05,
      "loss": 0.8575,
      "step": 400
    },
    {
      "epoch": 0.5832147937411095,
      "grad_norm": 15.556222915649414,
      "learning_rate": 4.09e-05,
      "loss": 0.7963,
      "step": 410
    },
    {
      "epoch": 0.5974395448079659,
      "grad_norm": 10.091852188110352,
      "learning_rate": 4.19e-05,
      "loss": 0.8975,
      "step": 420
    },
    {
      "epoch": 0.6116642958748222,
      "grad_norm": 13.677718162536621,
      "learning_rate": 4.29e-05,
      "loss": 0.7774,
      "step": 430
    },
    {
      "epoch": 0.6258890469416786,
      "grad_norm": 10.118350982666016,
      "learning_rate": 4.39e-05,
      "loss": 0.877,
      "step": 440
    },
    {
      "epoch": 0.6401137980085349,
      "grad_norm": 9.817999839782715,
      "learning_rate": 4.49e-05,
      "loss": 0.819,
      "step": 450
    },
    {
      "epoch": 0.6543385490753911,
      "grad_norm": 24.589834213256836,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.8056,
      "step": 460
    },
    {
      "epoch": 0.6685633001422475,
      "grad_norm": 8.711180686950684,
      "learning_rate": 4.69e-05,
      "loss": 0.7938,
      "step": 470
    },
    {
      "epoch": 0.6827880512091038,
      "grad_norm": 9.455667495727539,
      "learning_rate": 4.79e-05,
      "loss": 0.7622,
      "step": 480
    },
    {
      "epoch": 0.6970128022759602,
      "grad_norm": 14.439214706420898,
      "learning_rate": 4.89e-05,
      "loss": 0.7761,
      "step": 490
    },
    {
      "epoch": 0.7112375533428165,
      "grad_norm": 10.26200008392334,
      "learning_rate": 4.99e-05,
      "loss": 0.8221,
      "step": 500
    },
    {
      "epoch": 0.7254623044096729,
      "grad_norm": 9.072775840759277,
      "learning_rate": 4.9931087289433386e-05,
      "loss": 0.8503,
      "step": 510
    },
    {
      "epoch": 0.7396870554765291,
      "grad_norm": 8.588502883911133,
      "learning_rate": 4.9854517611026036e-05,
      "loss": 0.8276,
      "step": 520
    },
    {
      "epoch": 0.7539118065433855,
      "grad_norm": 10.704909324645996,
      "learning_rate": 4.9777947932618686e-05,
      "loss": 0.8504,
      "step": 530
    },
    {
      "epoch": 0.7681365576102418,
      "grad_norm": 13.403555870056152,
      "learning_rate": 4.9701378254211336e-05,
      "loss": 0.8184,
      "step": 540
    },
    {
      "epoch": 0.7823613086770982,
      "grad_norm": 8.108067512512207,
      "learning_rate": 4.962480857580398e-05,
      "loss": 0.8152,
      "step": 550
    },
    {
      "epoch": 0.7965860597439545,
      "grad_norm": 12.912999153137207,
      "learning_rate": 4.954823889739663e-05,
      "loss": 0.7915,
      "step": 560
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 9.75949478149414,
      "learning_rate": 4.947166921898928e-05,
      "loss": 0.8226,
      "step": 570
    },
    {
      "epoch": 0.8250355618776671,
      "grad_norm": 13.370220184326172,
      "learning_rate": 4.939509954058193e-05,
      "loss": 0.82,
      "step": 580
    },
    {
      "epoch": 0.8392603129445235,
      "grad_norm": 14.065789222717285,
      "learning_rate": 4.931852986217458e-05,
      "loss": 0.8213,
      "step": 590
    },
    {
      "epoch": 0.8534850640113798,
      "grad_norm": 10.430328369140625,
      "learning_rate": 4.924196018376723e-05,
      "loss": 0.7856,
      "step": 600
    },
    {
      "epoch": 0.8677098150782361,
      "grad_norm": 11.928752899169922,
      "learning_rate": 4.916539050535988e-05,
      "loss": 0.7786,
      "step": 610
    },
    {
      "epoch": 0.8819345661450925,
      "grad_norm": 10.451881408691406,
      "learning_rate": 4.908882082695253e-05,
      "loss": 0.7404,
      "step": 620
    },
    {
      "epoch": 0.8961593172119487,
      "grad_norm": 11.33611011505127,
      "learning_rate": 4.901225114854518e-05,
      "loss": 0.7228,
      "step": 630
    },
    {
      "epoch": 0.9103840682788051,
      "grad_norm": 11.351533889770508,
      "learning_rate": 4.8935681470137825e-05,
      "loss": 0.8028,
      "step": 640
    },
    {
      "epoch": 0.9246088193456614,
      "grad_norm": 9.705218315124512,
      "learning_rate": 4.8859111791730476e-05,
      "loss": 0.7436,
      "step": 650
    },
    {
      "epoch": 0.9388335704125178,
      "grad_norm": 15.73327922821045,
      "learning_rate": 4.8782542113323126e-05,
      "loss": 0.7511,
      "step": 660
    },
    {
      "epoch": 0.9530583214793741,
      "grad_norm": 11.100709915161133,
      "learning_rate": 4.8705972434915776e-05,
      "loss": 0.7929,
      "step": 670
    },
    {
      "epoch": 0.9672830725462305,
      "grad_norm": 10.06485366821289,
      "learning_rate": 4.8629402756508426e-05,
      "loss": 0.7677,
      "step": 680
    },
    {
      "epoch": 0.9815078236130867,
      "grad_norm": 6.817529201507568,
      "learning_rate": 4.855283307810107e-05,
      "loss": 0.8248,
      "step": 690
    },
    {
      "epoch": 0.9957325746799431,
      "grad_norm": 10.331194877624512,
      "learning_rate": 4.847626339969372e-05,
      "loss": 0.8192,
      "step": 700
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.8123738169670105,
      "eval_runtime": 21.8912,
      "eval_samples_per_second": 256.633,
      "eval_steps_per_second": 2.01,
      "step": 703
    },
    {
      "epoch": 1.0099573257467995,
      "grad_norm": 12.900344848632812,
      "learning_rate": 4.839969372128637e-05,
      "loss": 0.7305,
      "step": 710
    },
    {
      "epoch": 1.0241820768136558,
      "grad_norm": 12.193281173706055,
      "learning_rate": 4.832312404287902e-05,
      "loss": 0.641,
      "step": 720
    },
    {
      "epoch": 1.038406827880512,
      "grad_norm": 15.015323638916016,
      "learning_rate": 4.824655436447167e-05,
      "loss": 0.622,
      "step": 730
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 11.841290473937988,
      "learning_rate": 4.816998468606432e-05,
      "loss": 0.7475,
      "step": 740
    },
    {
      "epoch": 1.0668563300142249,
      "grad_norm": 11.804483413696289,
      "learning_rate": 4.809341500765697e-05,
      "loss": 0.7044,
      "step": 750
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 11.91377067565918,
      "learning_rate": 4.801684532924962e-05,
      "loss": 0.6782,
      "step": 760
    },
    {
      "epoch": 1.0953058321479374,
      "grad_norm": 11.109296798706055,
      "learning_rate": 4.794027565084227e-05,
      "loss": 0.7313,
      "step": 770
    },
    {
      "epoch": 1.1095305832147937,
      "grad_norm": 16.720090866088867,
      "learning_rate": 4.786370597243492e-05,
      "loss": 0.6856,
      "step": 780
    },
    {
      "epoch": 1.12375533428165,
      "grad_norm": 11.972296714782715,
      "learning_rate": 4.7787136294027566e-05,
      "loss": 0.668,
      "step": 790
    },
    {
      "epoch": 1.1379800853485065,
      "grad_norm": 14.987053871154785,
      "learning_rate": 4.7710566615620216e-05,
      "loss": 0.598,
      "step": 800
    },
    {
      "epoch": 1.1522048364153628,
      "grad_norm": 10.628617286682129,
      "learning_rate": 4.7633996937212866e-05,
      "loss": 0.6184,
      "step": 810
    },
    {
      "epoch": 1.166429587482219,
      "grad_norm": 11.703505516052246,
      "learning_rate": 4.755742725880552e-05,
      "loss": 0.7106,
      "step": 820
    },
    {
      "epoch": 1.1806543385490753,
      "grad_norm": 11.766924858093262,
      "learning_rate": 4.748085758039817e-05,
      "loss": 0.7412,
      "step": 830
    },
    {
      "epoch": 1.1948790896159318,
      "grad_norm": 10.81338119506836,
      "learning_rate": 4.740428790199081e-05,
      "loss": 0.6657,
      "step": 840
    },
    {
      "epoch": 1.209103840682788,
      "grad_norm": 13.901564598083496,
      "learning_rate": 4.732771822358346e-05,
      "loss": 0.6357,
      "step": 850
    },
    {
      "epoch": 1.2233285917496444,
      "grad_norm": 19.151052474975586,
      "learning_rate": 4.725114854517611e-05,
      "loss": 0.6649,
      "step": 860
    },
    {
      "epoch": 1.2375533428165006,
      "grad_norm": 15.049888610839844,
      "learning_rate": 4.717457886676876e-05,
      "loss": 0.6833,
      "step": 870
    },
    {
      "epoch": 1.251778093883357,
      "grad_norm": 10.269196510314941,
      "learning_rate": 4.709800918836141e-05,
      "loss": 0.5963,
      "step": 880
    },
    {
      "epoch": 1.2660028449502134,
      "grad_norm": 11.28846263885498,
      "learning_rate": 4.702143950995406e-05,
      "loss": 0.7039,
      "step": 890
    },
    {
      "epoch": 1.2802275960170697,
      "grad_norm": 10.555949211120605,
      "learning_rate": 4.694486983154671e-05,
      "loss": 0.708,
      "step": 900
    },
    {
      "epoch": 1.294452347083926,
      "grad_norm": 16.18883514404297,
      "learning_rate": 4.686830015313936e-05,
      "loss": 0.6301,
      "step": 910
    },
    {
      "epoch": 1.3086770981507825,
      "grad_norm": 12.22950267791748,
      "learning_rate": 4.679173047473201e-05,
      "loss": 0.6518,
      "step": 920
    },
    {
      "epoch": 1.3229018492176388,
      "grad_norm": 10.950868606567383,
      "learning_rate": 4.6715160796324656e-05,
      "loss": 0.6566,
      "step": 930
    },
    {
      "epoch": 1.337126600284495,
      "grad_norm": 11.467315673828125,
      "learning_rate": 4.6638591117917306e-05,
      "loss": 0.6593,
      "step": 940
    },
    {
      "epoch": 1.3513513513513513,
      "grad_norm": 13.947294235229492,
      "learning_rate": 4.6562021439509956e-05,
      "loss": 0.6143,
      "step": 950
    },
    {
      "epoch": 1.3655761024182076,
      "grad_norm": 13.72728157043457,
      "learning_rate": 4.648545176110261e-05,
      "loss": 0.6499,
      "step": 960
    },
    {
      "epoch": 1.379800853485064,
      "grad_norm": 11.68026065826416,
      "learning_rate": 4.640888208269526e-05,
      "loss": 0.7246,
      "step": 970
    },
    {
      "epoch": 1.3940256045519204,
      "grad_norm": 10.237722396850586,
      "learning_rate": 4.63323124042879e-05,
      "loss": 0.7759,
      "step": 980
    },
    {
      "epoch": 1.4082503556187767,
      "grad_norm": 7.415541172027588,
      "learning_rate": 4.625574272588055e-05,
      "loss": 0.6794,
      "step": 990
    },
    {
      "epoch": 1.422475106685633,
      "grad_norm": 15.549039840698242,
      "learning_rate": 4.61791730474732e-05,
      "loss": 0.667,
      "step": 1000
    },
    {
      "epoch": 1.4366998577524894,
      "grad_norm": 14.040182113647461,
      "learning_rate": 4.610260336906585e-05,
      "loss": 0.5949,
      "step": 1010
    },
    {
      "epoch": 1.4509246088193457,
      "grad_norm": 10.959790229797363,
      "learning_rate": 4.60260336906585e-05,
      "loss": 0.6766,
      "step": 1020
    },
    {
      "epoch": 1.465149359886202,
      "grad_norm": 11.603554725646973,
      "learning_rate": 4.594946401225115e-05,
      "loss": 0.5803,
      "step": 1030
    },
    {
      "epoch": 1.4793741109530583,
      "grad_norm": 18.972412109375,
      "learning_rate": 4.58728943338438e-05,
      "loss": 0.6682,
      "step": 1040
    },
    {
      "epoch": 1.4935988620199145,
      "grad_norm": 16.158933639526367,
      "learning_rate": 4.579632465543645e-05,
      "loss": 0.6483,
      "step": 1050
    },
    {
      "epoch": 1.5078236130867708,
      "grad_norm": 8.538803100585938,
      "learning_rate": 4.57197549770291e-05,
      "loss": 0.7117,
      "step": 1060
    },
    {
      "epoch": 1.5220483641536273,
      "grad_norm": 10.099020957946777,
      "learning_rate": 4.5643185298621746e-05,
      "loss": 0.7755,
      "step": 1070
    },
    {
      "epoch": 1.5362731152204836,
      "grad_norm": 13.68813419342041,
      "learning_rate": 4.5566615620214396e-05,
      "loss": 0.6141,
      "step": 1080
    },
    {
      "epoch": 1.55049786628734,
      "grad_norm": 13.752535820007324,
      "learning_rate": 4.5490045941807047e-05,
      "loss": 0.6337,
      "step": 1090
    },
    {
      "epoch": 1.5647226173541964,
      "grad_norm": 15.662435531616211,
      "learning_rate": 4.54134762633997e-05,
      "loss": 0.6543,
      "step": 1100
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 17.731626510620117,
      "learning_rate": 4.533690658499235e-05,
      "loss": 0.6182,
      "step": 1110
    },
    {
      "epoch": 1.593172119487909,
      "grad_norm": 14.647388458251953,
      "learning_rate": 4.526033690658499e-05,
      "loss": 0.681,
      "step": 1120
    },
    {
      "epoch": 1.6073968705547652,
      "grad_norm": 17.051149368286133,
      "learning_rate": 4.518376722817764e-05,
      "loss": 0.6981,
      "step": 1130
    },
    {
      "epoch": 1.6216216216216215,
      "grad_norm": 10.590269088745117,
      "learning_rate": 4.510719754977029e-05,
      "loss": 0.7452,
      "step": 1140
    },
    {
      "epoch": 1.635846372688478,
      "grad_norm": 10.731423377990723,
      "learning_rate": 4.503062787136294e-05,
      "loss": 0.6179,
      "step": 1150
    },
    {
      "epoch": 1.6500711237553343,
      "grad_norm": 12.151172637939453,
      "learning_rate": 4.495405819295559e-05,
      "loss": 0.7291,
      "step": 1160
    },
    {
      "epoch": 1.6642958748221908,
      "grad_norm": 15.125164031982422,
      "learning_rate": 4.487748851454824e-05,
      "loss": 0.6398,
      "step": 1170
    },
    {
      "epoch": 1.678520625889047,
      "grad_norm": 13.847535133361816,
      "learning_rate": 4.480091883614089e-05,
      "loss": 0.6823,
      "step": 1180
    },
    {
      "epoch": 1.6927453769559033,
      "grad_norm": 17.743345260620117,
      "learning_rate": 4.472434915773354e-05,
      "loss": 0.6396,
      "step": 1190
    },
    {
      "epoch": 1.7069701280227596,
      "grad_norm": 13.157424926757812,
      "learning_rate": 4.464777947932619e-05,
      "loss": 0.7284,
      "step": 1200
    },
    {
      "epoch": 1.7211948790896159,
      "grad_norm": 13.244355201721191,
      "learning_rate": 4.4571209800918836e-05,
      "loss": 0.678,
      "step": 1210
    },
    {
      "epoch": 1.7354196301564722,
      "grad_norm": 12.471867561340332,
      "learning_rate": 4.4494640122511486e-05,
      "loss": 0.6951,
      "step": 1220
    },
    {
      "epoch": 1.7496443812233284,
      "grad_norm": 10.962291717529297,
      "learning_rate": 4.441807044410414e-05,
      "loss": 0.6971,
      "step": 1230
    },
    {
      "epoch": 1.763869132290185,
      "grad_norm": 10.806594848632812,
      "learning_rate": 4.434150076569679e-05,
      "loss": 0.6108,
      "step": 1240
    },
    {
      "epoch": 1.7780938833570412,
      "grad_norm": 15.169886589050293,
      "learning_rate": 4.426493108728944e-05,
      "loss": 0.7086,
      "step": 1250
    },
    {
      "epoch": 1.7923186344238977,
      "grad_norm": 14.5762939453125,
      "learning_rate": 4.418836140888208e-05,
      "loss": 0.6775,
      "step": 1260
    },
    {
      "epoch": 1.806543385490754,
      "grad_norm": 11.436555862426758,
      "learning_rate": 4.411179173047473e-05,
      "loss": 0.6621,
      "step": 1270
    },
    {
      "epoch": 1.8207681365576103,
      "grad_norm": 10.120556831359863,
      "learning_rate": 4.403522205206738e-05,
      "loss": 0.7741,
      "step": 1280
    },
    {
      "epoch": 1.8349928876244666,
      "grad_norm": 15.72159194946289,
      "learning_rate": 4.395865237366003e-05,
      "loss": 0.669,
      "step": 1290
    },
    {
      "epoch": 1.8492176386913228,
      "grad_norm": 10.146339416503906,
      "learning_rate": 4.388208269525268e-05,
      "loss": 0.6543,
      "step": 1300
    },
    {
      "epoch": 1.863442389758179,
      "grad_norm": 11.753511428833008,
      "learning_rate": 4.380551301684533e-05,
      "loss": 0.704,
      "step": 1310
    },
    {
      "epoch": 1.8776671408250356,
      "grad_norm": 9.726566314697266,
      "learning_rate": 4.372894333843798e-05,
      "loss": 0.6428,
      "step": 1320
    },
    {
      "epoch": 1.8918918918918919,
      "grad_norm": 14.689205169677734,
      "learning_rate": 4.365237366003063e-05,
      "loss": 0.6859,
      "step": 1330
    },
    {
      "epoch": 1.9061166429587484,
      "grad_norm": 12.094243049621582,
      "learning_rate": 4.357580398162328e-05,
      "loss": 0.6992,
      "step": 1340
    },
    {
      "epoch": 1.9203413940256047,
      "grad_norm": 14.306281089782715,
      "learning_rate": 4.349923430321593e-05,
      "loss": 0.6242,
      "step": 1350
    },
    {
      "epoch": 1.934566145092461,
      "grad_norm": 20.096189498901367,
      "learning_rate": 4.3422664624808577e-05,
      "loss": 0.6522,
      "step": 1360
    },
    {
      "epoch": 1.9487908961593172,
      "grad_norm": 15.849534034729004,
      "learning_rate": 4.334609494640123e-05,
      "loss": 0.6892,
      "step": 1370
    },
    {
      "epoch": 1.9630156472261735,
      "grad_norm": 15.811798095703125,
      "learning_rate": 4.326952526799388e-05,
      "loss": 0.6979,
      "step": 1380
    },
    {
      "epoch": 1.9772403982930298,
      "grad_norm": 10.461099624633789,
      "learning_rate": 4.319295558958653e-05,
      "loss": 0.6209,
      "step": 1390
    },
    {
      "epoch": 1.991465149359886,
      "grad_norm": 12.603464126586914,
      "learning_rate": 4.311638591117918e-05,
      "loss": 0.6486,
      "step": 1400
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.7155680656433105,
      "eval_runtime": 22.1394,
      "eval_samples_per_second": 253.755,
      "eval_steps_per_second": 1.987,
      "step": 1406
    },
    {
      "epoch": 2.0056899004267423,
      "grad_norm": 12.509268760681152,
      "learning_rate": 4.303981623277182e-05,
      "loss": 0.5864,
      "step": 1410
    },
    {
      "epoch": 2.019914651493599,
      "grad_norm": 11.973592758178711,
      "learning_rate": 4.296324655436447e-05,
      "loss": 0.4224,
      "step": 1420
    },
    {
      "epoch": 2.0341394025604553,
      "grad_norm": 25.70332145690918,
      "learning_rate": 4.288667687595712e-05,
      "loss": 0.4986,
      "step": 1430
    },
    {
      "epoch": 2.0483641536273116,
      "grad_norm": 20.379993438720703,
      "learning_rate": 4.281010719754977e-05,
      "loss": 0.4574,
      "step": 1440
    },
    {
      "epoch": 2.062588904694168,
      "grad_norm": 18.276329040527344,
      "learning_rate": 4.273353751914242e-05,
      "loss": 0.4163,
      "step": 1450
    },
    {
      "epoch": 2.076813655761024,
      "grad_norm": 20.115610122680664,
      "learning_rate": 4.265696784073507e-05,
      "loss": 0.4556,
      "step": 1460
    },
    {
      "epoch": 2.0910384068278804,
      "grad_norm": 14.476471900939941,
      "learning_rate": 4.258039816232772e-05,
      "loss": 0.4593,
      "step": 1470
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 13.216099739074707,
      "learning_rate": 4.250382848392037e-05,
      "loss": 0.3821,
      "step": 1480
    },
    {
      "epoch": 2.119487908961593,
      "grad_norm": 25.203723907470703,
      "learning_rate": 4.242725880551302e-05,
      "loss": 0.4374,
      "step": 1490
    },
    {
      "epoch": 2.1337126600284497,
      "grad_norm": 23.50241470336914,
      "learning_rate": 4.235068912710567e-05,
      "loss": 0.4262,
      "step": 1500
    },
    {
      "epoch": 2.147937411095306,
      "grad_norm": 18.709156036376953,
      "learning_rate": 4.227411944869832e-05,
      "loss": 0.5105,
      "step": 1510
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 17.6351375579834,
      "learning_rate": 4.219754977029097e-05,
      "loss": 0.3895,
      "step": 1520
    },
    {
      "epoch": 2.1763869132290186,
      "grad_norm": 18.050012588500977,
      "learning_rate": 4.212098009188362e-05,
      "loss": 0.4352,
      "step": 1530
    },
    {
      "epoch": 2.190611664295875,
      "grad_norm": 14.820865631103516,
      "learning_rate": 4.204441041347627e-05,
      "loss": 0.4915,
      "step": 1540
    },
    {
      "epoch": 2.204836415362731,
      "grad_norm": 19.789470672607422,
      "learning_rate": 4.196784073506891e-05,
      "loss": 0.447,
      "step": 1550
    },
    {
      "epoch": 2.2190611664295874,
      "grad_norm": 16.13593864440918,
      "learning_rate": 4.189127105666156e-05,
      "loss": 0.4352,
      "step": 1560
    },
    {
      "epoch": 2.2332859174964437,
      "grad_norm": 13.156188011169434,
      "learning_rate": 4.181470137825421e-05,
      "loss": 0.4838,
      "step": 1570
    },
    {
      "epoch": 2.2475106685633,
      "grad_norm": 21.13086700439453,
      "learning_rate": 4.173813169984686e-05,
      "loss": 0.4356,
      "step": 1580
    },
    {
      "epoch": 2.2617354196301562,
      "grad_norm": 13.517033576965332,
      "learning_rate": 4.166156202143951e-05,
      "loss": 0.4057,
      "step": 1590
    },
    {
      "epoch": 2.275960170697013,
      "grad_norm": 13.000898361206055,
      "learning_rate": 4.1584992343032156e-05,
      "loss": 0.3977,
      "step": 1600
    },
    {
      "epoch": 2.2901849217638692,
      "grad_norm": 9.387991905212402,
      "learning_rate": 4.150842266462481e-05,
      "loss": 0.364,
      "step": 1610
    },
    {
      "epoch": 2.3044096728307255,
      "grad_norm": 21.190048217773438,
      "learning_rate": 4.143185298621746e-05,
      "loss": 0.5344,
      "step": 1620
    },
    {
      "epoch": 2.318634423897582,
      "grad_norm": 15.251858711242676,
      "learning_rate": 4.135528330781011e-05,
      "loss": 0.4165,
      "step": 1630
    },
    {
      "epoch": 2.332859174964438,
      "grad_norm": 22.307144165039062,
      "learning_rate": 4.127871362940276e-05,
      "loss": 0.4516,
      "step": 1640
    },
    {
      "epoch": 2.3470839260312943,
      "grad_norm": 13.173629760742188,
      "learning_rate": 4.120214395099541e-05,
      "loss": 0.39,
      "step": 1650
    },
    {
      "epoch": 2.3613086770981506,
      "grad_norm": 19.545055389404297,
      "learning_rate": 4.112557427258806e-05,
      "loss": 0.4526,
      "step": 1660
    },
    {
      "epoch": 2.3755334281650073,
      "grad_norm": 21.828882217407227,
      "learning_rate": 4.104900459418071e-05,
      "loss": 0.4047,
      "step": 1670
    },
    {
      "epoch": 2.3897581792318636,
      "grad_norm": 11.688979148864746,
      "learning_rate": 4.097243491577336e-05,
      "loss": 0.5055,
      "step": 1680
    },
    {
      "epoch": 2.40398293029872,
      "grad_norm": 19.735004425048828,
      "learning_rate": 4.0895865237366e-05,
      "loss": 0.4722,
      "step": 1690
    },
    {
      "epoch": 2.418207681365576,
      "grad_norm": 16.656949996948242,
      "learning_rate": 4.081929555895865e-05,
      "loss": 0.4479,
      "step": 1700
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 17.443828582763672,
      "learning_rate": 4.07427258805513e-05,
      "loss": 0.4016,
      "step": 1710
    },
    {
      "epoch": 2.4466571834992887,
      "grad_norm": 19.56418228149414,
      "learning_rate": 4.066615620214395e-05,
      "loss": 0.4047,
      "step": 1720
    },
    {
      "epoch": 2.460881934566145,
      "grad_norm": 22.70697784423828,
      "learning_rate": 4.05895865237366e-05,
      "loss": 0.4802,
      "step": 1730
    },
    {
      "epoch": 2.4751066856330013,
      "grad_norm": 16.703821182250977,
      "learning_rate": 4.0513016845329246e-05,
      "loss": 0.4311,
      "step": 1740
    },
    {
      "epoch": 2.4893314366998576,
      "grad_norm": 28.67573356628418,
      "learning_rate": 4.04364471669219e-05,
      "loss": 0.4776,
      "step": 1750
    },
    {
      "epoch": 2.503556187766714,
      "grad_norm": 21.65573501586914,
      "learning_rate": 4.035987748851455e-05,
      "loss": 0.5151,
      "step": 1760
    },
    {
      "epoch": 2.5177809388335706,
      "grad_norm": 21.650291442871094,
      "learning_rate": 4.0283307810107203e-05,
      "loss": 0.4014,
      "step": 1770
    },
    {
      "epoch": 2.532005689900427,
      "grad_norm": 19.031633377075195,
      "learning_rate": 4.0206738131699854e-05,
      "loss": 0.4608,
      "step": 1780
    },
    {
      "epoch": 2.546230440967283,
      "grad_norm": 9.055821418762207,
      "learning_rate": 4.01301684532925e-05,
      "loss": 0.4163,
      "step": 1790
    },
    {
      "epoch": 2.5604551920341394,
      "grad_norm": 31.952396392822266,
      "learning_rate": 4.005359877488515e-05,
      "loss": 0.4757,
      "step": 1800
    },
    {
      "epoch": 2.5746799431009957,
      "grad_norm": 19.645679473876953,
      "learning_rate": 3.99770290964778e-05,
      "loss": 0.4519,
      "step": 1810
    },
    {
      "epoch": 2.588904694167852,
      "grad_norm": 14.943319320678711,
      "learning_rate": 3.990045941807045e-05,
      "loss": 0.4265,
      "step": 1820
    },
    {
      "epoch": 2.6031294452347082,
      "grad_norm": 11.56176471710205,
      "learning_rate": 3.98238897396631e-05,
      "loss": 0.4291,
      "step": 1830
    },
    {
      "epoch": 2.617354196301565,
      "grad_norm": 25.691438674926758,
      "learning_rate": 3.974732006125574e-05,
      "loss": 0.4205,
      "step": 1840
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 18.355823516845703,
      "learning_rate": 3.967075038284839e-05,
      "loss": 0.4584,
      "step": 1850
    },
    {
      "epoch": 2.6458036984352775,
      "grad_norm": 35.32711410522461,
      "learning_rate": 3.959418070444104e-05,
      "loss": 0.5883,
      "step": 1860
    },
    {
      "epoch": 2.660028449502134,
      "grad_norm": 17.525423049926758,
      "learning_rate": 3.951761102603369e-05,
      "loss": 0.5106,
      "step": 1870
    },
    {
      "epoch": 2.67425320056899,
      "grad_norm": 18.728214263916016,
      "learning_rate": 3.944104134762634e-05,
      "loss": 0.3981,
      "step": 1880
    },
    {
      "epoch": 2.6884779516358464,
      "grad_norm": 24.418163299560547,
      "learning_rate": 3.9364471669218986e-05,
      "loss": 0.4333,
      "step": 1890
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 26.125675201416016,
      "learning_rate": 3.928790199081164e-05,
      "loss": 0.4823,
      "step": 1900
    },
    {
      "epoch": 2.716927453769559,
      "grad_norm": 22.56610679626465,
      "learning_rate": 3.9211332312404294e-05,
      "loss": 0.4364,
      "step": 1910
    },
    {
      "epoch": 2.731152204836415,
      "grad_norm": 21.988935470581055,
      "learning_rate": 3.9134762633996944e-05,
      "loss": 0.4857,
      "step": 1920
    },
    {
      "epoch": 2.7453769559032715,
      "grad_norm": 13.978023529052734,
      "learning_rate": 3.905819295558959e-05,
      "loss": 0.4824,
      "step": 1930
    },
    {
      "epoch": 2.759601706970128,
      "grad_norm": 17.00491714477539,
      "learning_rate": 3.898162327718224e-05,
      "loss": 0.4313,
      "step": 1940
    },
    {
      "epoch": 2.7738264580369845,
      "grad_norm": 12.762351036071777,
      "learning_rate": 3.890505359877489e-05,
      "loss": 0.4703,
      "step": 1950
    },
    {
      "epoch": 2.7880512091038407,
      "grad_norm": 17.89883804321289,
      "learning_rate": 3.882848392036754e-05,
      "loss": 0.4524,
      "step": 1960
    },
    {
      "epoch": 2.802275960170697,
      "grad_norm": 18.930011749267578,
      "learning_rate": 3.875191424196019e-05,
      "loss": 0.4396,
      "step": 1970
    },
    {
      "epoch": 2.8165007112375533,
      "grad_norm": 20.5289363861084,
      "learning_rate": 3.867534456355283e-05,
      "loss": 0.4094,
      "step": 1980
    },
    {
      "epoch": 2.8307254623044096,
      "grad_norm": 17.001205444335938,
      "learning_rate": 3.859877488514548e-05,
      "loss": 0.4575,
      "step": 1990
    },
    {
      "epoch": 2.844950213371266,
      "grad_norm": 19.969335556030273,
      "learning_rate": 3.852220520673813e-05,
      "loss": 0.4961,
      "step": 2000
    },
    {
      "epoch": 2.8591749644381226,
      "grad_norm": 17.21376609802246,
      "learning_rate": 3.844563552833078e-05,
      "loss": 0.3993,
      "step": 2010
    },
    {
      "epoch": 2.873399715504979,
      "grad_norm": 24.308197021484375,
      "learning_rate": 3.836906584992343e-05,
      "loss": 0.3578,
      "step": 2020
    },
    {
      "epoch": 2.887624466571835,
      "grad_norm": 19.37323760986328,
      "learning_rate": 3.8292496171516076e-05,
      "loss": 0.4732,
      "step": 2030
    },
    {
      "epoch": 2.9018492176386914,
      "grad_norm": 21.429523468017578,
      "learning_rate": 3.821592649310873e-05,
      "loss": 0.4637,
      "step": 2040
    },
    {
      "epoch": 2.9160739687055477,
      "grad_norm": 24.49315071105957,
      "learning_rate": 3.8139356814701384e-05,
      "loss": 0.4912,
      "step": 2050
    },
    {
      "epoch": 2.930298719772404,
      "grad_norm": 13.173144340515137,
      "learning_rate": 3.8062787136294034e-05,
      "loss": 0.4357,
      "step": 2060
    },
    {
      "epoch": 2.9445234708392602,
      "grad_norm": 17.46622085571289,
      "learning_rate": 3.798621745788668e-05,
      "loss": 0.4589,
      "step": 2070
    },
    {
      "epoch": 2.9587482219061165,
      "grad_norm": 32.623844146728516,
      "learning_rate": 3.790964777947933e-05,
      "loss": 0.4439,
      "step": 2080
    },
    {
      "epoch": 2.972972972972973,
      "grad_norm": 19.777278900146484,
      "learning_rate": 3.783307810107198e-05,
      "loss": 0.3867,
      "step": 2090
    },
    {
      "epoch": 2.987197724039829,
      "grad_norm": 20.65458106994629,
      "learning_rate": 3.775650842266463e-05,
      "loss": 0.5084,
      "step": 2100
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.7397493720054626,
      "eval_runtime": 22.2028,
      "eval_samples_per_second": 253.031,
      "eval_steps_per_second": 1.982,
      "step": 2109
    }
  ],
  "logging_steps": 10,
  "max_steps": 7030,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 402800497065216.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
